{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609bcc26",
   "metadata": {},
   "source": [
    "# Assignment 2 - Feature extraction and classification\n",
    "\n",
    "In this assignment, you are expected to\n",
    "\n",
    "(1) extract global features from a publicly available dataset with one of the pre-trained neural networks available in pytorch, \n",
    "\n",
    "and \n",
    "\n",
    "(2) classify the dataset using the traditional k-Neural Neighbours classifier.\n",
    "\n",
    "You are also asked to implement k-fold cross-validation to evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b394f7",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4d6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load needed packages\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838957a",
   "metadata": {},
   "source": [
    "When working with Pytorch, dataloader() is a must to know function.\n",
    "\n",
    "Read more about this function and the parameters it accepts in https://blog.paperspace.com/dataloaders-abstractions-pytorch/ ;\n",
    "\n",
    "DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7db27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2320b0",
   "metadata": {},
   "source": [
    "The variable transform encapsulates the needed transformations of our data\n",
    "\n",
    "Read more about transforms in https://blog.paperspace.com/dataloaders-abstractions-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3daee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # resize\n",
    "    transforms.Resize(256),\n",
    "    # center-crop\n",
    "    transforms.CenterCrop(224),\n",
    "    # to-tensor\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d746beb",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f0e38",
   "metadata": {},
   "source": [
    "# INPUT DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f4cd8",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ea732",
   "metadata": {},
   "source": [
    "Load your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53bb75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CIFAR10'\n",
    "classes = ('plane', 'car', 'bird', \n",
    "           'cat','deer', 'dog', 'frog', \n",
    "           'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbaf5a8d-38ad-4c44-98d5-bfec3b549d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = torchvision.datasets.CIFAR10(root='./datasets', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c0aa35-9594-4f75-9beb-ea81d5966dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49139968 0.48215841 0.44653091]\n",
      "[0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.data.mean(axis=(0,1,2))/255)\n",
    "print(dataset.data.std(axis=(0,1,2))/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4f51b7-be6b-4c04-9e6c-53fb7ca4e8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='./datasets', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(root='./datasets', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb189026",
   "metadata": {},
   "source": [
    "## Exercise: Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57aaaa8",
   "metadata": {},
   "source": [
    "### Train - Test Split\n",
    "\n",
    "Write a function **train_test_split(dataset, ratio)** which takes a dataset as an input and returns two datasets one for training and another for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b5f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, ratio):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    indices = [i for i in range(len(dataset))]\n",
    "    np.random.shuffle(indices)\n",
    "    max_ind = int(ratio*len(dataset))\n",
    "    for i in range(max_ind):\n",
    "        X_train.append(dataset[indices[i]][0])\n",
    "        y_train.append(dataset[indices[i]][1])\n",
    "    for i in range(len(dataset)-max_ind):\n",
    "        X_test.append(dataset[indices[i+max_ind]][0])\n",
    "        y_test.append(dataset[indices[i+max_ind]][1])\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ed70b7-6e15-4b92-90dd-41b325a046b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset,0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f7feb",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f160f",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION\n",
    "\n",
    "Extract descriptros from the images in your train and test dataset. The dataset split should remain the same for all the experiments if you want to be fair when comparing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f0878",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032fccdb",
   "metadata": {},
   "source": [
    "## Exercise: Feature 1 - RGB descriptors\n",
    "\n",
    "Following the code you implement in Assignment 1, extract R, G, and B descriptors from the images and concatenate them to create a 1D feature vector of 24 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f654967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_descriptor(img):\n",
    "    img = np.array(img)\n",
    "    r = img[:,:,0]\n",
    "    g = img[:,:,1]\n",
    "    b = img[:,:,2]\n",
    "    r_bins = np.array(np.histogram(r,7)[1])\n",
    "    g_bins = np.array(np.histogram(g,7)[1])\n",
    "    b_bins = np.array(np.histogram(b,7)[1])\n",
    "    return(list(r_bins)+list(g_bins)+list(b_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e29bf76-00f0-4665-b0fa-25a2d832c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptors = []\n",
    "test_descriptors = []\n",
    "for i in range(len(X_train)):\n",
    "    train_descriptors.append(make_descriptor(X_train[i]))\n",
    "for i in range(len(X_test)):\n",
    "    test_descriptors.append(make_descriptor(X_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51372d11",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise: Feature 2 - Extract descriptors using pre-trained networks\n",
    "\n",
    "Load a pre-trained network to extract global features from the images. \n",
    "We will use the values of the last fully connected layer of the deep network as a descriptor, i.e. we will remove the last fully-connected layer. Therefore, after feed-forwarding the input through the network, we save the output as the descriptor of the image.\n",
    "\n",
    "You can use different networks for this purpose.\n",
    "\n",
    "Reading material to start with;\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#sphx-glr-beginner-transfer-learning-tutorial-py\n",
    "\n",
    "https://medium.com/analytics-vidhya/cnn-transfer-learning-with-vgg-16-and-resnet-50-feature-extraction-for-image-retrieval-with-keras-53320c580853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9618d68-6815-4ef7-812b-0e435fb4ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.49139968, 0.48215841, 0.44653091])\n",
    "    std = np.array([0.24703223, 0.24348513, 0.26158784])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e04373c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee723401-da58-43e9-a1f4-86906f746c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_alexnet = nn.Sequential(*list(alexnet.classifier.children())[:-1])\n",
    "alexnet.classifier = new_alexnet\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8e832",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599bb2b",
   "metadata": {},
   "source": [
    "# PERFORMANCE EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304d19d",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a59fdf",
   "metadata": {},
   "source": [
    "## Exercise: Error function\n",
    "\n",
    "Implement a function to evaluate the accuracy of your prediction. \n",
    "We will rely on the evaluation metric accuracy.\n",
    "\n",
    "You are suggested to also use f-score, recall and precision. Have a look at https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8067316-2173-4853-b120-e3c3a342cdf0",
   "metadata": {},
   "source": [
    "Let's use the following notations : \n",
    "- TP = True Positive\n",
    "- FP = False Positive\n",
    "- TN = True Negative\n",
    "- FN = False Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680e86d-0ee2-4ff4-b8b4-7c64ac357940",
   "metadata": {},
   "source": [
    "$$ accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$ \\\n",
    "$$ precision_i = \\frac{TP}{TP + FP} $$ \\\n",
    "$$ precision = \\frac{1}{n}\\sum_{i=1}^n precision_i $$ \\\n",
    "$$ recall_i = \\frac{TP}{TP + FN} $$ \\\n",
    "$$ recall = \\frac{1}{n}\\sum_{i=1}^n recall_i $$ \\\n",
    "$$ fscore = 2 \\cdot \\frac{recall \\cdot precision}{recall + precision} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3677bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "    return sum(np.equal(actual,predicted))/len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dee4ec7-5942-4005-ad79-fd54d5213570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_metric(actual, predicted, n_classes=10):\n",
    "    p = 0\n",
    "    for i in range(n_classes):\n",
    "        actual = np.array(np.equal(actual, i), dtype=int)\n",
    "        predicted = np.array(np.equal(predicted, i), dtype=int)\n",
    "        tp = np.sum(np.equal(actual+predicted,2))\n",
    "        fp = np.sum(np.equal(predicted-actual,1))\n",
    "        if (tp != 0):\n",
    "            p += tp/(tp+fp)\n",
    "    return p/n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb0f6fed-49d9-4014-abb4-2ef3b5685a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_metric(actual, predicted, n_classes=10):\n",
    "    r = 0\n",
    "    for i in range(n_classes):\n",
    "        actual = np.array(np.equal(actual, i), dtype=int)\n",
    "        predicted = np.array(np.equal(predicted, i), dtype=int)\n",
    "        tp = np.sum(np.equal(actual+predicted,2))\n",
    "        fn = np.sum(np.equal(predicted-actual,-1))\n",
    "        if (tp != 0):\n",
    "            r += tp/(tp+fn)\n",
    "    return r/n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63259fe5-6504-4210-ae12-22d510e71f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fscore_metric(actual, predicted, n_classes=10):\n",
    "    r = recall_metric(actual, predicted, n_classes)\n",
    "    p = precision_metric(actual, predicted, n_classes)\n",
    "    return 2*r*p/(r+p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549727ac",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b447784",
   "metadata": {},
   "source": [
    "# TRAIN AND TEST YOUR MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c6375",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab251607",
   "metadata": {},
   "source": [
    "## Exercise: k Nearest Neighbour model\n",
    "\n",
    "Apply the classifier with different values of k (number of nearest neighbours) to the two sets of previously extracted descriptors (RGB and CNN features) and evaluate the performance of your models (accuracy).\n",
    "\n",
    "You can have a look at the documentation to understand the parameters that define the learning of the model,\n",
    "https://scikit-learn.org/stable/modules/neighbors.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e657e-d5b4-46cf-a1eb-13188d057a53",
   "metadata": {},
   "source": [
    "### RGB features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1470832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b90aba2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KNeighborsClassifier(n_neighbors=2),\n",
       " KNeighborsClassifier(n_neighbors=4),\n",
       " KNeighborsClassifier(n_neighbors=6),\n",
       " KNeighborsClassifier(n_neighbors=10),\n",
       " KNeighborsClassifier(n_neighbors=15)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use your k-NN - play with the value of the parameters to see how the model performs\n",
    "kvalue_list = [2,4,6,10,15] \n",
    "neighbors = [KNeighborsClassifier(n_neighbors=kvalue_list[i]).fit(train_descriptors,y_train) for i in range(len(kvalue_list))]\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc108288-359b-48ad-ae76-42400fa6d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [neighbors[i].predict(test_descriptors) for i in range(len(neighbors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5529dacd-fc61-438e-9c32-5e7d5b407ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = [accuracy_metric(y_test,pred[i]) for i in range(len(pred))]\n",
    "plt.plot(kvalue_list,acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab9af70d-ae70-4a57-b719-b363ed5b481d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1643, 0.1821, 0.1912, 0.1977, 0.2067]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837ea76-d167-4f10-8868-a2ecc141beeb",
   "metadata": {},
   "source": [
    "### CNN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1255be18-d536-4b74-87c8-f3d3367cccaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "alexnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4fb7c4a-3781-4ef2-928b-ca0ad82bf85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6165b8b-fd5b-4516-af15-325516e2a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training of AlexNet\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = alexnet(inputs)\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print('Finished Training of AlexNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbe4f56d-654c-42a6-845f-c6b3607aefea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5, 1, 7], device='cuda:0')\n",
      "tensor([3, 5, 1, 7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = alexnet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(predicted)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e94c23",
   "metadata": {},
   "source": [
    "## Exercise: Visualize results \n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "1) Apply PCA and select the 2 first principal components to represent each sample.\n",
    "\n",
    "2) Plot the samples with dots. Use a colour per class. \n",
    "\n",
    "3) Plot the samples again but with empty filled circles. Use the color of the class predicted per sample (misclassifications will make the colours not coincide).\n",
    "\n",
    "You can do this for (1) training and (2) test set. In (1) you can see how well the method fits the training data and (2) will give you an idea of the misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cbfb5",
   "metadata": {},
   "source": [
    "## Exercise: kNN with k-Fold cross-validation\n",
    "\n",
    "Assess the performance of your implemented kNN using k-Fold cross-validation. \n",
    "\n",
    "Run your implemented function evaluating for k (fold) = 2, 5 and 10. You can rely on the kNN that performed best in the previous exercises.\n",
    "Report the average accuracy and the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f603d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba435dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUGGESTION ON HOW TO PRESENT PERFORMANCE OF YOUR KFOLD CROSS VALIDATION ANALYSIS\n",
    "\n",
    "print('Summary results:')\n",
    "print(' ')\n",
    "print(' ')\n",
    "for i,k in enumerate(k_list):\n",
    "    print(k,'-fold cross validation:')  \n",
    "    print('Accuracies per fold: ', avg_acc_list[i]) \n",
    "    \n",
    "    avg_acc = round(sum(avg_acc_list[i])/k,2)\n",
    "    std_list= round(np.std(avg_acc_list[i]),2)\n",
    "    print('Average accuracy: ', avg_acc,'+-', std_list) \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf70eb",
   "metadata": {},
   "source": [
    "### [Optional] Exercise: further explore by: \n",
    "- implement other classifiers such as SVM or Random Forest, \n",
    "- extract other descriptors from the images such as objects or other local features,\n",
    "- implement the evaluation metrics: recall, precission and f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b69360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
